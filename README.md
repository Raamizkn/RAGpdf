# Personal Research Assistant for PDFs (RAG-Powered)

A powerful AI-powered research assistant that can analyze multiple PDFs, extract information, and answer queries using Retrieval-Augmented Generation (RAG).

## Features

- **Multi-PDF Upload**: Upload multiple research papers, legal documents, or books
- **Advanced Chunking**: Uses NLP-based chunking instead of naive fixed-length chunks
- **RAG-Powered Retrieval**: Retrieves the most relevant text and augments LLM responses
- **Local & API-Based LLMs**: Works with both local models (Llama 2, Mistral 7B) and API-based models
- **Hybrid Search**: Combines embeddings and BM25 for improved retrieval accuracy
- **Citation Tracking**: Shows source references for extracted answers
- **Scalability**: Can process hundreds of PDFs efficiently
- **Simple Web UI**: User-friendly Streamlit-based frontend

## Setup and Installation

1. Clone this repository
2. Install dependencies:
   ```
   pip install -r requirements.txt
   ```
3. For local LLM support, install [Ollama](https://ollama.ai/) and download a model:
   ```
   ollama pull mistral
   ```

## Usage

1. Start the backend server:
   ```
   cd backend
   uvicorn main:app --reload
   ```

2. Start the Streamlit frontend:
   ```
   cd frontend
   streamlit run app.py
   ```

3. Open your browser and navigate to http://localhost:8501

## Project Structure

```
ğŸ“‚ research-assistant
â”œâ”€â”€ ğŸ“ backend
â”‚   â”œâ”€â”€ main.py  # FastAPI server
â”‚   â”œâ”€â”€ process_pdf.py  # PDF extraction & chunking
â”‚   â”œâ”€â”€ embed_store.py  # Vector DB handling (ChromaDB/FAISS)
â”‚   â”œâ”€â”€ query_handler.py  # RAG retrieval & LLM response
â”‚   â”œâ”€â”€ config.py  # Configuration settings
â”œâ”€â”€ ğŸ“ frontend
â”‚   â”œâ”€â”€ app.py  # Streamlit UI
â”œâ”€â”€ ğŸ“ models
â”‚   â”œâ”€â”€ embeddings.pkl  # Precomputed embeddings
â”œâ”€â”€ ğŸ“ data
â”‚   â”œâ”€â”€ sample_papers.pdf
â”œâ”€â”€ requirements.txt  # Dependencies
â”œâ”€â”€ README.md  # Project documentation
```

## License

MIT 